{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e3c2a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 2])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MLPPlanner(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_track: int = 10,\n",
    "        n_waypoints: int = 3,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_track (int): number of points in each side of the track\n",
    "            n_waypoints (int): number of waypoints to predict\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_track = n_track\n",
    "        self.n_waypoints = n_waypoints\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2 * n_track * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_waypoints * 2),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        track_left: torch.Tensor,\n",
    "        track_right: torch.Tensor,\n",
    "        **kwargs,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predicts waypoints from the left and right boundaries of the track.\n",
    "\n",
    "        During test time, your model will be called with\n",
    "        model(track_left=..., track_right=...), so keep the function signature as is.\n",
    "\n",
    "        Args:\n",
    "            track_left (torch.Tensor): shape (b, n_track, 2)\n",
    "            track_right (torch.Tensor): shape (b, n_track, 2)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: future waypoints with shape (b, n_waypoints, 2)\n",
    "        \"\"\"\n",
    "        b = track_left.shape[0]\n",
    "        x = torch.cat([track_left, track_right], dim=1)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.model(x)\n",
    "        return x.view(b, self.n_waypoints, 2)\n",
    "\n",
    "left = torch.randn(3, 10, 2).to(\"cuda\")\n",
    "right = torch.randn(3, 10, 2).to(\"cuda\")\n",
    "net = MLPPlanner().to(\"cuda\")\n",
    "net(left, right).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01ed1501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 40])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left = torch.randn(3, 10, 2).to(\"cuda\")\n",
    "right = torch.randn(3, 10, 2).to(\"cuda\")\n",
    "x = torch.cat([left, right], dim=1)\n",
    "x = x.flatten(start_dim=1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb03d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 2])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerPlanner(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_track: int = 10,\n",
    "        n_waypoints: int = 3,\n",
    "        d_model: int = 64,\n",
    "        n_head: int = 4,\n",
    "        num_layers: int = 2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_track = n_track\n",
    "        self.n_waypoints = n_waypoints\n",
    "\n",
    "        self.query_embed = nn.Embedding(n_waypoints, d_model)\n",
    "\n",
    "        self.input_proj = nn.Linear(2, d_model)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=n_head, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # 4. An output head to project from d_model to 2D waypoints\n",
    "        self.output_head = nn.Linear(d_model, 2)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        track_left: torch.Tensor,\n",
    "        track_right: torch.Tensor,\n",
    "        **kwargs,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predicts waypoints from the left and right boundaries of the track.\n",
    "\n",
    "        During test time, your model will be called with\n",
    "        model(track_left=..., track_right=...), so keep the function signature as is.\n",
    "\n",
    "        Args:\n",
    "            track_left (torch.Tensor): shape (b, n_track, 2)\n",
    "            track_right (torch.Tensor): shape (b, n_track, 2)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: future waypoints with shape (b, n_waypoints, 2)\n",
    "        \"\"\"\n",
    "        b = track_left.shape[0]\n",
    "\n",
    "        # (n_waypoints, d_model) -> (1, n_waypoints, d_model) -> (b, n_waypoints, d_model)\n",
    "        # create embedding for possible waypoint positions\n",
    "        queries = self.query_embed.weight.unsqueeze(0).repeat(b, 1, 1)\n",
    "        # print(queries.shape)\n",
    "\n",
    "        track_points = torch.cat([track_left, track_right], dim=1)  # (b, 2 * n_track, 2)\n",
    "        # print(track_points.shape)\n",
    "        memory = self.input_proj(track_points)  # (b, 2 * n_track, d_model)\n",
    "        # print(memory.shape)\n",
    "\n",
    "        output = self.decoder(tgt=queries, memory=memory) # (b, n_waypoints, d_model)\n",
    "        # print(output.shape)\n",
    "\n",
    "        # 4. Get final waypoints\n",
    "        waypoints = self.output_head(output) # (b, n_waypoints, 2)\n",
    "\n",
    "        return waypoints\n",
    "\n",
    "net = TransformerPlanner().to(\"cuda\")\n",
    "net(left, right).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc4f0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 2])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_MEAN = [0.2788, 0.2657, 0.2629]\n",
    "INPUT_STD = [0.2064, 0.1944, 0.2252]\n",
    "class CNNPlanner(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_waypoints: int = 3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_waypoints = n_waypoints\n",
    "\n",
    "        self.register_buffer(\"input_mean\", torch.as_tensor(INPUT_MEAN), persistent=False)\n",
    "        self.register_buffer(\"input_std\", torch.as_tensor(INPUT_STD), persistent=False)\n",
    "\n",
    "        num_channels = 32\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            nn.Conv2d(3, num_channels, kernel_size = 3, stride = 2, padding = 1, bias = False),\n",
    "            nn.BatchNorm2d(num_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_channels, num_channels * 2, kernel_size=3, stride = 2, padding = 1, bias=False), # 32x64\n",
    "            nn.BatchNorm2d(num_channels * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_channels * 2, num_channels * 4, kernel_size=3, stride = 2, padding = 1, bias=False), # 64x128\n",
    "            nn.BatchNorm2d(num_channels * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_channels * 4, num_channels * 8, kernel_size=3, stride = 2, padding = 1, bias=False), # 64x128\n",
    "            nn.BatchNorm2d(num_channels * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "\n",
    "        self.outputs = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, self.n_waypoints * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, image: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image (torch.FloatTensor): shape (b, 3, h, w) and vals in [0, 1]\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: future waypoints with shape (b, n, 2)\n",
    "        \"\"\"\n",
    "        b = image.shape[0]\n",
    "        x = (image - self.input_mean[None, :, None, None]) / self.input_std[None, :, None, None]\n",
    "\n",
    "        features = self.blocks(x)\n",
    "        # print(features.shape)\n",
    "\n",
    "        output = self.outputs(features)\n",
    "        \n",
    "        return output.view(b, self.n_waypoints, 2)\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "x = torch.randn(1, 3, 96, 128).to(\"cuda\")\n",
    "net = CNNPlanner().to(\"cuda\")\n",
    "net(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a420e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8000 samples from 16 episodes\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'tuple' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 28\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# torch.manual_seed(seed)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# np.random.seed(seed)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Use 'state_only' for MLP and Transformer planners, 'default' for CNN planner\u001b[39;00m\n\u001b[0;32m     26\u001b[0m transform_pipeline \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcnn_planner\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_only\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 28\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrive_data/train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_pipeline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtransform_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m load_data(\n\u001b[0;32m     36\u001b[0m     dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrive_data/val\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     37\u001b[0m     transform_pipeline \u001b[38;5;241m=\u001b[39m transform_pipeline,\n\u001b[0;32m     38\u001b[0m     num_workers \u001b[38;5;241m=\u001b[39m num_workers,\n\u001b[0;32m     39\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m batch_size,\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     42\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(model_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mg:\\My Drive\\Masters\\A I W394D - Deep Learning\\deep_learning\\homework4\\homework\\datasets\\road_dataset.py:114\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(dataset_path, transform_pipeline, return_dataloader, num_workers, batch_size, shuffle)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dataloader:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\logan\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:262\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    241\u001b[0m     dataset: Dataset[_T_co],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m     in_order: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m ):\n\u001b[0;32m    260\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython.data_loader\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnum_workers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m:\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    264\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_workers option should be non-negative; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    265\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse num_workers=0 to disable multiprocessing.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m         )\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'tuple' and 'int'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from homework.models import MLPPlanner, TransformerPlanner, CNNPlanner, load_model, save_model\n",
    "from homework.metrics import PlannerMetric\n",
    "from homework.datasets.road_dataset import load_data\n",
    "\n",
    "exp_dir: str = \"logs\",\n",
    "model_name: str = \"mlp_planner\",\n",
    "num_epoch: int = 1,\n",
    "lr: float = 1e-3,\n",
    "batch_size: int = 128,\n",
    "seed: int = 2024,\n",
    "num_workers = 0,\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# torch.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "\n",
    "# Use 'state_only' for MLP and Transformer planners, 'default' for CNN planner\n",
    "transform_pipeline = \"default\" if model_name == \"cnn_planner\" else \"state_only\"\n",
    "\n",
    "train_loader = load_data(\n",
    "    dataset_path= \"drive_data/train\",\n",
    "    transform_pipeline = transform_pipeline,\n",
    "    num_workers = num_workers,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    ")\n",
    "# val_loader = load_data(\n",
    "#     dataset_path = f\"drive_data/val\",\n",
    "#     transform_pipeline = transform_pipeline,\n",
    "#     num_workers = num_workers,\n",
    "#     batch_size = batch_size,\n",
    "# )\n",
    "\n",
    "model = load_model(model_name, **kwargs)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "criterion = nn.MSELoss(reduction='none') \n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "metric = PlannerMetric()\n",
    "\n",
    "best_val_error = float('inf')\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epoch):\n",
    "    train_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epoch} [Train]\")\n",
    "            \n",
    "    for batch in pbar:\n",
    "        # print(batch[\"waypoints_mask\"])\n",
    "        for k, v in batch.items():\n",
    "            print(k)\n",
    "        return\n",
    "        # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "        # predictions = model(batch[\"left_track\"], batch[\"right_track\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
